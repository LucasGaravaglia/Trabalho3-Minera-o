{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u0wBaS8R2j0"
      },
      "source": [
        "# **Trabalho 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uyPj6tmR6OM"
      },
      "source": [
        "Todos os anos centenas de filmes são lançados no mundo todo por diversas produtoras que contam com orçamentos de todos os tamanhos, gerando uma indústria bilhonária. Nesse contexto, você como cientista da computação recém formado foi contratado para monitorar em tempo real as impressões da platéia por meio de posts em redes sociais.\n",
        "\n",
        "Basicamente o seu trabalho é monitorar as impressões positivas e negativas de um determinado filme, nos últimos dias (exemplo: 30 dias). Você poderá fazer isso lendo todas as postagens e interpretando cada uma como positiva ou negativa ou **usando** o conhecimento adquirido durante a graduação e, portanto, automatizar essa tarefa. **Faça a escolha correta** e automatize essa tarefa. \n",
        "\n",
        "Para auxiliar nessa tarefa, considere o dataset anterior e os conceitos estudados em sala de aula.\n",
        "\n",
        "Escolha três filmes* já lançados de sua preferência para conduzir essa análise. Lembre-se que seu trabalho é apresentar uma forma automática de entender impressões positivas e negativas sobre os filmes escolhidos a partir de posts em uma rede social (no caso, o Twitter).\n",
        "\n",
        "O que você precisa apresentar é:\n",
        "\n",
        "+ uma análise sobre os posts referentes aos filmes escolhidos (isto é, o notebbok contendo o código e comentado. Seja organizado). Por exemplo, espera-se que no mínimo você apresente um gráfico de impressões positivas e negativas sobre cada filme escolhido nos últimos **n** dias.\n",
        "\n",
        "+ fazer um vídeo de no máximo 5 minutos explicando as tuas principais estratégias.\n",
        "\n",
        "**Data de entrega:** 10 de agosto de 2022\n",
        "\n",
        "*Seja prudente na escolha dos filmes, de modo que haja comentários suficientes para a sua análise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQE7qlxp0Myo"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GW-SiBuRbfh",
        "outputId": "4d2dc57a-783d-4579-99a1-68a030447a40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ktrain\n",
            "  Downloading ktrain-0.31.7.tar.gz (25.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.2.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.3.5)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ktrain) (21.3)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 62.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.42.1)\n",
            "Collecting cchardet\n",
            "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
            "\u001b[K     |████████████████████████████████| 263 kB 69.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.0.4)\n",
            "Collecting syntok>1.3.3\n",
            "  Downloading syntok-1.4.4-py3-none-any.whl (24 kB)\n",
            "Collecting transformers==4.17.0\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 36.8 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 55.4 MB/s \n",
            "\u001b[?25hCollecting keras_bert>=0.86.0\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[K     |████████████████████████████████| 468 kB 44.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (2022.6.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 52.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 27.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (4.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0->ktrain) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0->ktrain) (4.1.1)\n",
            "Collecting keras-transformer==0.40.0\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "Collecting keras-pos-embd==0.13.0\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "Collecting keras-multi-head==0.29.0\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "Collecting keras-layer-normalization==0.16.0\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "Collecting keras-position-wise-feed-forward==0.8.0\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "Collecting keras-embed-sim==0.10.0\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "Collecting keras-self-attention==0.51.0\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->ktrain) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->ktrain) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.17.0->ktrain) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.17.0->ktrain) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ktrain) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ktrain) (1.7.3)\n",
            "Building wheels for collected packages: ktrain, keras-bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, langdetect, sacremoses\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.31.7-py3-none-any.whl size=25312842 sha256=4282da5908a7e7fca0d8d28454e25f1c01366b881e4015dafcbe4a8aa4bbfc4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/98/8e/ce355dcb92451e85fab93f7ea2da068843e93e703928cd06fb\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33517 sha256=be26e3f90e367ea0fec6db0f1b6701adbd5989625df28494166873db88b049b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/e8/45/842b3a39831261aef9154b907eacbc4ac99499a99ae829b06f\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12305 sha256=a94c8e939ad3c6c2e3762137d725b5a665e3f2bd439df503838117e2ab936f07\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/68/26/692ed21edd832833c3b0a0e21615bcacd99ca458b3f9ed571f\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3960 sha256=4c21b08b9d5efec68dcba7d6aed5f5b9a5a27061be9b70d55c1452111f33ae59\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/67/b5/d847588d075895281e1cf5590f819bd4cf076a554872268bd5\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=df19d377930c45ae9cce8539941bfe18e83ff31770c72ebaa08a9e6787801b4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/5d/1c/2e619f594f69fbcf8bc20943b27d414871c409be053994813e\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14993 sha256=a2e033fc0b4cbb7ae619610a05917bf3e6b6df532b21b1f3e9f97b6d857d5575\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/aa/3c/9d15d24005179dae08ff291ce99c754b296347817d076fd9fb\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6962 sha256=ab5f4554e74d6f379000c443fd2e702ba7ee6ed4798c222688f63eb013c198a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/c1/a0/dc44fcf68c857b7ff6be9a97e675e5adf51022eff1169b042f\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4983 sha256=aa9a34ca7c8988c6e2fe29884229065c2092af898c048b3091bb7f9c64485e9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/75/6f/d42f6e051506f442daeba53ff1e2d21a5f20ef8c411610f2bb\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=8e0d5211ad6215c336a73ae860fdd56f6d7429eaaa1ae8fd4f1fbf47a97689be\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=7222c30d1abac83be4c703f844fc37148846cc571ca88ce519d187e20f2c0d1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=c4e3405e44fd0fa6e842899807c7c42f4bec90ef51c6882bbb4c3d58c7f31cda\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built ktrain keras-bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention langdetect sacremoses\n",
            "Installing collected packages: keras-self-attention, pyyaml, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, tokenizers, sacremoses, keras-transformer, huggingface-hub, whoosh, transformers, syntok, sentencepiece, langdetect, keras-bert, cchardet, ktrain\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed cchardet-2.1.7 huggingface-hub-0.8.1 keras-bert-0.89.0 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 ktrain-0.31.7 langdetect-1.0.9 pyyaml-6.0 sacremoses-0.0.53 sentencepiece-0.1.97 syntok-1.4.4 tokenizers-0.12.1 transformers-4.17.0 whoosh-2.7.4\n"
          ]
        }
      ],
      "source": [
        "!pip install ktrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t3WyPlkVBdlq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import keras\n",
        "import ktrain\n",
        "import json\n",
        "import textblob\n",
        "import wordcloud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re    \n",
        "import tweepy\n",
        "from ktrain import text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oOzTtzvVBnYz"
      },
      "outputs": [],
      "source": [
        "s = requests.get('https://raw.githubusercontent.com/ragero/text-collections/master/complete_texts_csvs/review_polarity.csv').content\n",
        "\n",
        "dataset = pd.read_csv(io.StringIO(s.decode('utf-8')),\n",
        "                na_values=['?'],\n",
        "                skipinitialspace = True)\n",
        "dataset = dataset.sample(frac=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "OFWo51t8-p5E",
        "outputId": "4d909a10-0b6b-489f-d4c3-2e790c6a75b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['neg', 'pos']\n",
            "      neg  pos\n",
            "1278  0.0  1.0\n",
            "685   1.0  0.0\n",
            "1892  0.0  1.0\n",
            "1243  0.0  1.0\n",
            "541   1.0  0.0\n",
            "['neg', 'pos']\n",
            "      neg  pos\n",
            "1431  0.0  1.0\n",
            "1556  0.0  1.0\n",
            "601   1.0  0.0\n",
            "300   1.0  0.0\n",
            "569   1.0  0.0\n",
            "downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n",
            "[██████████████████████████████████████████████████]\n",
            "extracting pretrained BERT model...\n",
            "done.\n",
            "\n",
            "cleanup downloaded zip...\n",
            "done.\n",
            "\n",
            "preprocessing train...\n",
            "language: en\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "preprocessing test...\n",
            "language: en\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test), preprocess = text.texts_from_df(dataset, \n",
        "                                                                   'text',\n",
        "                                                                   label_columns='class',\n",
        "                                                                   maxlen=64, \n",
        "                                                                   max_features=50000,\n",
        "                                                                   preprocess_mode='bert',\n",
        "                                                                   lang=None,\n",
        "                                                                   val_pct = 0.1,\n",
        "                                                                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omxb016W-3s_",
        "outputId": "3b1cc18c-5059-453d-aae4-a0e33a8ee94d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 64\n",
            "done.\n"
          ]
        }
      ],
      "source": [
        "model = text.text_classifier('bert', (X_train, y_train) , preproc=preprocess)\n",
        "classifier = ktrain.get_learner(model, \n",
        "                             train_data=(X_train, y_train), \n",
        "                             val_data=(X_test, y_test),\n",
        "                             batch_size=64\n",
        "                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6zpKBhR_AN9",
        "outputId": "de2878c7-f619-4fd8-93d3-54c45047c613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 2e-05...\n",
            "Epoch 1/8\n",
            "29/29 [==============================] - 24s 821ms/step - loss: 0.1841 - accuracy: 0.9433 - val_loss: 0.8306 - val_accuracy: 0.6500\n",
            "Epoch 2/8\n",
            "29/29 [==============================] - 24s 836ms/step - loss: 0.1222 - accuracy: 0.9678 - val_loss: 0.9131 - val_accuracy: 0.6400\n",
            "Epoch 3/8\n",
            "29/29 [==============================] - 24s 827ms/step - loss: 0.0772 - accuracy: 0.9806 - val_loss: 1.1322 - val_accuracy: 0.6250\n",
            "Epoch 4/8\n",
            "29/29 [==============================] - 24s 818ms/step - loss: 0.1155 - accuracy: 0.9594 - val_loss: 1.2301 - val_accuracy: 0.6550\n",
            "Epoch 5/8\n",
            "29/29 [==============================] - 24s 822ms/step - loss: 0.0616 - accuracy: 0.9806 - val_loss: 1.3984 - val_accuracy: 0.6350\n",
            "Epoch 6/8\n",
            "29/29 [==============================] - 24s 824ms/step - loss: 0.0467 - accuracy: 0.9867 - val_loss: 1.2605 - val_accuracy: 0.6500\n",
            "Epoch 7/8\n",
            "29/29 [==============================] - 24s 822ms/step - loss: 0.0496 - accuracy: 0.9811 - val_loss: 1.2404 - val_accuracy: 0.6600\n",
            "Epoch 8/8\n",
            "29/29 [==============================] - 24s 821ms/step - loss: 0.0281 - accuracy: 0.9928 - val_loss: 1.2500 - val_accuracy: 0.6600\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8d1a5f16d0>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.fit_onecycle(0.00002,8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qvVCDjV_D0l",
        "outputId": "81276c93-1e1c-4372-93b6-6a070be0e181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.63      0.65       102\n",
            "           1       0.64      0.69      0.67        98\n",
            "\n",
            "    accuracy                           0.66       200\n",
            "   macro avg       0.66      0.66      0.66       200\n",
            "weighted avg       0.66      0.66      0.66       200\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[64, 38],\n",
              "       [30, 68]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.validate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8sIIcuB_P0W"
      },
      "source": [
        "## Aplicando a novos reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "khoAS5_O_Wd6"
      },
      "outputs": [],
      "source": [
        "predictor = ktrain.get_predictor(classifier.model, preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "siqLiuHt_av-",
        "outputId": "e107bc3b-ed1f-4f7a-88d9-10a623b82e7f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'pos'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictor.predict('this is a good movie')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N6bOFlr0VEfL"
      },
      "outputs": [],
      "source": [
        "log=pd.read_csv('keys.csv')\n",
        "consumerkey=log['key'][0]\n",
        "consumersecret=log['key'][1]\n",
        "accesstoken=log['key'][2]\n",
        "accesstokensecret=log['key'][3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "wGpH5LeAVCR1"
      },
      "outputs": [],
      "source": [
        "client = tweepy.Client(accesstoken)\n",
        "\n",
        "#Lendo twetts dos filmes thor amor e trovao, Sonic e Elvis.\n",
        "res1 =  tweepy.Paginator(client.search_recent_tweets, query=\"thorloveandthunder\", tweet_fields=['context_annotations', 'created_at'], max_results=100).flatten(limit=200)\n",
        "res2 =  tweepy.Paginator(client.search_recent_tweets, query=\"Sonic\", tweet_fields=['context_annotations', 'created_at'], max_results=100).flatten(limit=200)\n",
        "res3 =  tweepy.Paginator(client.search_recent_tweets, query=\"elvis\", tweet_fields=['context_annotations', 'created_at'], max_results=100).flatten(limit=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_caracteres(text,carc):\n",
        "    for i in range(len(carc)):\n",
        "            text = text.replace(carc[i],\"\")\n",
        "    return text\n",
        "\n",
        "def remove_stop_words(text,stops):\n",
        "    for i in stops:\n",
        "            idx = text.find(i)\n",
        "            if(idx != -1):\n",
        "                text = text[0:idx]+text[(idx+len(i)):len(text)]\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "T8LLBKSuVG9L"
      },
      "outputs": [],
      "source": [
        "#Guardando os twetts em im arquivo json\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.update([\"RT\",\"thor\",\"thunder\",\"ThorLoveAndThunder\",\"https\",\"\\n\"])\n",
        "caracteres = \"#@:;|_{}[]()\"\n",
        "info = {\"thor\":{\"text\":[]},\"Sonic\":{\"text\":[]},\"elvis\":{\"text\":[]}}\n",
        "with open('movies.json', 'w') as outfile:\n",
        "    for response in res1:\n",
        "        response.text = remove_caracteres(response.text,caracteres)\n",
        "        response.text = remove_stop_words(response.text,stopwords)\n",
        "        info[\"thor\"][\"text\"].append(response.text)\n",
        "    for response2 in res2:\n",
        "        response2.text = remove_caracteres(response2.text,caracteres)\n",
        "        response2.text = remove_stop_words(response2.text,stopwords)\n",
        "        info[\"Sonic\"][\"text\"].append(response2.text)\n",
        "    for response3 in res3:\n",
        "        response3.text = remove_caracteres(response3.text,caracteres)\n",
        "        response3.text = remove_stop_words(response3.text,stopwords)\n",
        "        info[\"elvis\"][\"text\"].append(response3.text)\n",
        "    outfile.write(json.dumps(info))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGmiAFzp-2BF"
      },
      "outputs": [],
      "source": [
        "# Word Cloud com os textos dos tweets\n",
        "wordcloud = WordCloud(width=1600, stopwords=stopwords,height=800,max_font_size=200,max_words=20,collocations=False, background_color='white').generate(string)\n",
        "plt.figure(figsize=(40,30))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE8SydlgedWc"
      },
      "source": [
        "## **Notebook útil e bem interessante que pode ajudar no trabalho**\n",
        "https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.ipynb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Trabalho_3_Lucas.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
